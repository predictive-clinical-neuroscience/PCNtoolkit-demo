{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T17:56:57.496449Z",
     "start_time": "2020-02-28T17:56:57.486640Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats, linalg\n",
    "from sklearn import preprocessing, decomposition, linear_model, metrics \n",
    "import warnings"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T21:49:43.572309Z",
     "start_time": "2020-02-27T21:49:43.566414Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# set fontsizes for matplotlib plots\n",
    "baseline_fontsize = 12\n",
    "SMALL_SIZE = 8 + baseline_fontsize\n",
    "MEDIUM_SIZE = 10 + baseline_fontsize\n",
    "BIGGER_SIZE = 12 + baseline_fontsize\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T18:47:50.679060Z",
     "start_time": "2020-02-28T18:47:50.664890Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_subs_T1 = np.load('/net/parasite/HCP/Subjects/saige_temp_HCP/resampled/all_subs_T1_array.npy')\n",
    "ages = np.load('/net/parasite/HCP/Subjects/saige_temp_HCP/resampled/age_labels.npy')\n",
    "gscores = np.load('/net/parasite/HCP/Subjects/saige_temp_HCP/resampled/g_labels.npy')"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T20:46:55.631289Z",
     "start_time": "2020-02-27T20:46:54.423706Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(all_subs_T1.shape)\n",
    "print(ages.shape)\n",
    "print(gscores.shape)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T20:46:55.650333Z",
     "start_time": "2020-02-27T20:46:55.638448Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocess Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_subs_T1_flat = all_subs_T1.reshape((all_subs_T1.shape[0], all_subs_T1.shape[1]*all_subs_T1.shape[2]*all_subs_T1.shape[3]))  # all_subs_T1_flat: (n_subjects, n_voxels)\n",
    "print(all_subs_T1_flat.shape)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T18:06:30.160969Z",
     "start_time": "2020-02-28T18:06:30.150093Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Train/Test Splits"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# generate train/test splits\n",
    "np.random.seed(42)\n",
    "n_train = int(0.8 * all_subs_T1_flat.shape[0])\n",
    "\n",
    "train_idxs = np.random.choice(range(all_subs_T1_flat.shape[0]), size=n_train, replace=False)\n",
    "test_idxs = np.array([x for x in range(all_subs_T1_flat.shape[0]) if x not in train_idxs])"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T18:06:31.317947Z",
     "start_time": "2020-02-28T18:06:31.286075Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_T1 = all_subs_T1_flat[train_idxs, :]\n",
    "test_T1 = all_subs_T1_flat[test_idxs, :]\n",
    "\n",
    "train_phen = ages[train_idxs]\n",
    "test_phen = ages[test_idxs]"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T19:27:03.079155Z",
     "start_time": "2020-02-28T19:27:02.301318Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# mean center train/test data (using train means)\n",
    "train_T1_mu_centered = (train_T1 - train_T1.mean(axis=0))\n",
    "test_T1_mu_centered = (test_T1 - train_T1.mean(axis=0))"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T18:06:33.825306Z",
     "start_time": "2020-02-28T18:06:32.401774Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Principal Component Regression (BBS)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pca_model = decomposition.PCA(n_components=75).fit(train_T1)\n",
    "# from pca documentation, \"the input data is centered but not scaled for each feature before applying the SVD\""
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T20:47:22.849443Z",
     "start_time": "2020-02-27T20:47:03.933802Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(f'First PC explains {pca_model.explained_variance_ratio_[0]*100:.2f}% of the total variance.\\nThis is an artifact of zero inflated T1 data')\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.bar(range(1, 51), pca_model.explained_variance_ratio_[1:51])\n",
    "plt.title('Variance Explained Ratio\\nPCs 1-50', fontsize=25)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T18:47:54.691293Z",
     "start_time": "2020-02-28T18:47:53.816603Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_T1_transformed = pca_model.transform(train_T1)\n",
    "test_T1_transformed = pca_model.transform(test_T1)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T20:47:25.521344Z",
     "start_time": "2020-02-27T20:47:23.812540Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fit Linear Regression Model "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# fast OLS using matrix math\n",
    "# we will check that this matches sklearn results later\n",
    "\n",
    "# fit ols model on dimension reduced train data\n",
    "train_features = np.hstack([np.ones((train_T1_transformed.shape[0], 1)), \n",
    "                            train_T1_transformed])\n",
    "train_features_inv = linalg.pinv2(train_features)\n",
    "train_betas = np.dot(train_features_inv, train_phen)\n",
    "train_pred_phen = np.dot(train_features, train_betas)\n",
    "\n",
    "# fit ols model on dimension reduced test data\n",
    "test_features = np.hstack([np.ones((test_T1_transformed.shape[0], 1)), \n",
    "                           test_T1_transformed])\n",
    "test_pred_phen = np.dot(test_features, train_betas)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T21:02:52.201826Z",
     "start_time": "2020-02-28T21:02:52.178135Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# OLS using sklearn\n",
    "\n",
    "lr_model = linear_model.LinearRegression(fit_intercept=True, normalize=False)\n",
    "lr_model.fit(train_T1_transformed, train_phen)\n",
    "train_pred_phen_lr_model = lr_model.predict(train_T1_transformed)\n",
    "test_pred_phen_lr_model = lr_model.predict(test_T1_transformed)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T21:02:52.511013Z",
     "start_time": "2020-02-28T21:02:52.496799Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ensure matrix math predictions and sklearn predictions are accurate to 5 decimals\n",
    "assert np.allclose(np.round(train_pred_phen - train_pred_phen_lr_model, 5), 0), 'Failed'\n",
    "assert np.allclose(np.round(test_pred_phen - test_pred_phen_lr_model, 5), 0), 'Failed'\n",
    "print('Passed')"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T21:02:53.178960Z",
     "start_time": "2020-02-28T21:02:53.170282Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Accuracy of Predictions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_r2 = metrics.r2_score(train_phen, train_pred_phen_lr_model)\n",
    "train_mae = metrics.mean_absolute_error(train_phen, train_pred_phen_lr_model)\n",
    "test_mae = metrics.mean_absolute_error(test_phen, test_pred_phen_lr_model)\n",
    "train_mae = metrics.mean_squared_error(train_phen, train_pred_phen_lr_model)\n",
    "test_mae = metrics.mean_squared_error(test_phen, test_pred_phen_lr_model)\n",
    "print(f'Train R^2: {train_r2:.3f}')\n",
    "print(f'Train MAE: {train_mae:.3f}')\n",
    "print(f'Test MAE: {test_mae:.3f}')\n",
    "print(f'Train MSE: {train_mae:.3f}')\n",
    "print(f'Test MSE: {test_mae:.3f}')"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T21:03:07.671439Z",
     "start_time": "2020-02-28T21:03:07.658887Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BBS Cross Validation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def bbs(X, y, n_components, n_cv_splits, pred_summary_function, verbose=False):\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    \n",
    "    fold_accs_train = []\n",
    "    fold_accs_test = []\n",
    "    np.random.seed(42)\n",
    "    shuffled_idxs = np.random.choice(range(X.shape[0]), size=X.shape[0], replace=False)\n",
    "    for fold_i, test_idxs in enumerate(np.array_split(shuffled_idxs, n_cv_splits)):\n",
    "        train_mask = np.ones(X.shape[0], np.bool)\n",
    "        train_mask[test_idxs] = 0\n",
    "\n",
    "        # create train/text X, y\n",
    "        train_X, test_X = X[train_mask, :], X[test_idxs, :]\n",
    "        train_y, test_y = y[train_mask], y[test_idxs]  \n",
    "\n",
    "        # mean center columns using train data only\n",
    "        train_X_mu = train_X.mean(axis=0)\n",
    "        train_X = train_X - train_X_mu\n",
    "        test_X = test_X - train_X_mu\n",
    "\n",
    "        # fit pca\n",
    "        if verbose:\n",
    "            print(f'CV Fold: {fold_i+1:<10} Fitting PCA model...')\n",
    "        pca_model = decomposition.PCA(n_components=n_components).fit(train_X)\n",
    "\n",
    "        # dimension reduce train/test data\n",
    "        train_X = pca_model.transform(train_X)\n",
    "        test_X = pca_model.transform(test_X)\n",
    "\n",
    "        # fit OLS model\n",
    "        if verbose:\n",
    "            print(f'CV Fold: {fold_i+1:<10} Fitting Linear Regression model...')\n",
    "        lr_model = linear_model.LinearRegression(fit_intercept=True, normalize=False)\n",
    "        lr_model.fit(train_X, train_y)\n",
    "\n",
    "        train_pred = lr_model.predict(train_X)\n",
    "        test_pred = lr_model.predict(test_X)\n",
    "\n",
    "        fold_accs_train.append(pred_summary_function(train_y, train_pred))\n",
    "        fold_accs_test.append(pred_summary_function(test_y, test_pred))\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'CV Fold: {fold_i+1:<10} Train Accuracy: {round(fold_accs_train[-1], 3):<10} Test Accuracy: {round(fold_accs_test[-1], 3):<10}')\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(13, 7))\n",
    "    plt.plot(range(1, len(fold_accs_train)+1), fold_accs_train, linestyle='-', marker='o', color='C0', label='Train CV Performance')\n",
    "    plt.plot(range(1, len(fold_accs_test)+1), fold_accs_test, linestyle='-', marker='o', color='C1', label='Test CV Performance')\n",
    "    plt.title(pred_summary_function.__name__, fontsize=20)\n",
    "    plt.xticks(range(1, len(fold_accs_test)+1))\n",
    "    plt.xlabel('CV Fold')\n",
    "    plt.legend(fontsize=20)\n",
    "    plt.show()\n",
    "    \n",
    "    return fold_accs_train, fold_accs_test"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T20:47:30.836578Z",
     "start_time": "2020-02-27T20:47:30.818844Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fold_accs_train, fold_accs_test = bbs(all_subs_T1_flat, ages, n_components=75, n_cv_splits=5, pred_summary_function=metrics.mean_absolute_error, verbose=True)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T18:49:39.777131Z",
     "start_time": "2020-02-28T18:47:58.929094Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Connectome Predictive Modelling "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# correlation train_brain with train_phenotype\n",
    "train_pheno_corr_p = [stats.pearsonr(train_T1[:, i], train_phen) for i in range(train_T1.shape[1])]  # train_pheno_corr_p: (259200, )\n",
    "# there are some nan correlations if brain data is poorly cropped (ie: some columns are always 0)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T21:24:31.288665Z",
     "start_time": "2020-02-27T21:23:55.467812Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# split into positive and negative correlations \n",
    "# and keep edges with p values below threshold\n",
    "pval_threshold = 0.01\n",
    "\n",
    "train_corrs = np.array([x[0] for x in train_pheno_corr_p])\n",
    "train_pvals = np.array([x[1] for x in train_pheno_corr_p])\n",
    "\n",
    "keep_edges_pos = (train_corrs > 0) & (train_pvals < pval_threshold)\n",
    "keep_edges_neg = (train_corrs < 0) & (train_pvals < pval_threshold)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T21:24:34.704689Z",
     "start_time": "2020-02-27T21:24:34.501675Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(f'number of positive edges kept = {np.sum(keep_edges_pos)}')\n",
    "print(f'number of negative edges kept = {np.sum(keep_edges_neg)}')"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T21:26:12.022732Z",
     "start_time": "2020-02-27T21:26:12.013945Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_pos_edges_sum = train_T1[:, keep_edges_pos].sum(1)\n",
    "train_neg_edges_sum = train_T1[:, keep_edges_neg].sum(1)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T21:29:03.616394Z",
     "start_time": "2020-02-27T21:29:03.571004Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fit_pos = linear_model.LinearRegression(fit_intercept=True, normalize=False).fit(train_pos_edges_sum.reshape(-1, 1), train_phen)\n",
    "fit_neg = linear_model.LinearRegression(fit_intercept=True, normalize=False).fit(train_neg_edges_sum.reshape(-1, 1), train_phen)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T21:30:46.258668Z",
     "start_time": "2020-02-27T21:30:46.190775Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pos_error = metrics.mean_absolute_error(train_phen, fit_pos.predict(train_pos_edges_sum.reshape(-1, 1)))\n",
    "neg_error = metrics.mean_absolute_error(train_phen, fit_neg.predict(train_neg_edges_sum.reshape(-1, 1)))\n",
    "\n",
    "print(f'Training Error (Positive Edges Model) = {pos_error:.3f}')\n",
    "print(f'Training Error (Negative Edges Model) = {neg_error:.3f}')"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T21:33:41.567137Z",
     "start_time": "2020-02-27T21:33:41.557150Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# combine positive/negative edges in one linear regression model\n",
    "fit_pos_neg = linear_model.LinearRegression(fit_intercept=True, normalize=False).fit(np.stack((train_pos_edges_sum, train_neg_edges_sum)).T, train_phen)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T21:34:05.962843Z",
     "start_time": "2020-02-27T21:34:05.952234Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pos_neg_error = metrics.mean_absolute_error(train_phen, fit_pos_neg.predict(np.stack((train_pos_edges_sum, train_neg_edges_sum)).T))\n",
    "\n",
    "print(f'Training Error (Positive/Negative Edges Model) = {pos_neg_error:.3f}')"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T21:34:40.010030Z",
     "start_time": "2020-02-27T21:34:39.990469Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# evaluate out of sample performance \n",
    "test_pos_edges_sum = test_T1[:, keep_edges_pos].sum(1)\n",
    "test_neg_edges_sum = test_T1[:, keep_edges_neg].sum(1)\n",
    "\n",
    "pos_test_error = metrics.mean_absolute_error(test_phen, fit_pos.predict(test_pos_edges_sum.reshape(-1, 1)))\n",
    "neg_test_error = metrics.mean_absolute_error(test_phen, fit_neg.predict(test_neg_edges_sum.reshape(-1, 1)))\n",
    "pos_neg_test_error = metrics.mean_absolute_error(test_phen, fit_pos_neg.predict(np.stack((test_pos_edges_sum, test_neg_edges_sum)).T))\n",
    "\n",
    "print(f'Testing Error (Positive Edges Model) = {pos_test_error:.3f}')\n",
    "print(f'Testing Error (Negative Edges Model) = {neg_test_error:.3f}')\n",
    "print(f'Testing Error (Positive/Negative Edges Model) = {pos_neg_test_error:.3f}')"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T21:36:11.826718Z",
     "start_time": "2020-02-27T21:36:11.807247Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CPM Cross Validation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def cpm(X, y, p_threshold, n_cv_splits, pred_summary_function, verbose=False):\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    \n",
    "    fold_accs_train = []\n",
    "    fold_accs_test = []\n",
    "    np.random.seed(42)\n",
    "    shuffled_idxs = np.random.choice(range(X.shape[0]), size=X.shape[0], replace=False)\n",
    "    for fold_i, test_idxs in enumerate(np.array_split(shuffled_idxs, n_cv_splits)):\n",
    "        train_mask = np.ones(X.shape[0], np.bool)\n",
    "        train_mask[test_idxs] = 0\n",
    "\n",
    "        # create train/text X, y\n",
    "        train_X, test_X = X[train_mask, :], X[test_idxs, :]\n",
    "        train_y, test_y = y[train_mask], y[test_idxs]  \n",
    "        \n",
    "        # create correlation matrix between train_X and train_y\n",
    "        if verbose:\n",
    "            print(f'CV Fold: {fold_i+1:<10} Computing correlations between train_X and train_y...')\n",
    "        with warnings.catch_warnings():\n",
    "            # we expect pearsonr to throw PearsonRConstantInputWarning because of contant valued columns in X\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            train_pheno_corr_p = [stats.pearsonr(train_X[:, i], train_y) for i in range(train_X.shape[1])]\n",
    "            train_corrs = np.array([x[0] for x in train_pheno_corr_p])\n",
    "            train_pvals = np.array([x[1] for x in train_pheno_corr_p])\n",
    "            # create masks for edges below p-threshold and split pos/neg correlations\n",
    "            keep_edges_pos = (train_corrs > 0) & (train_pvals < p_threshold)\n",
    "            keep_edges_neg = (train_corrs < 0) & (train_pvals < p_threshold)\n",
    "        \n",
    "        # sum X entries with significant correlations with y\n",
    "        train_pos_edges_sum = train_X[:, keep_edges_pos].sum(1)\n",
    "        train_neg_edges_sum = train_X[:, keep_edges_neg].sum(1)\n",
    "        test_pos_edges_sum = test_X[:, keep_edges_pos].sum(1)\n",
    "        test_neg_edges_sum = test_X[:, keep_edges_neg].sum(1)\n",
    "        \n",
    "        # fit linear regression models based on summed values\n",
    "        fit_pos = linear_model.LinearRegression(fit_intercept=True, normalize=False).fit(train_pos_edges_sum.reshape(-1, 1), train_y)\n",
    "        fit_neg = linear_model.LinearRegression(fit_intercept=True, normalize=False).fit(train_neg_edges_sum.reshape(-1, 1), train_y)\n",
    "        fit_pos_neg = linear_model.LinearRegression(fit_intercept=True, normalize=False).fit(np.stack((train_pos_edges_sum, train_neg_edges_sum)).T, train_y)\n",
    "        \n",
    "        # compute train errors\n",
    "        train_pos_error = pred_summary_function(train_y, fit_pos.predict(train_pos_edges_sum.reshape(-1, 1)))\n",
    "        train_neg_error = pred_summary_function(train_y, fit_neg.predict(train_neg_edges_sum.reshape(-1, 1)))\n",
    "        train_posneg_error = pred_summary_function(train_y, fit_pos_neg.predict(np.stack((train_pos_edges_sum, train_neg_edges_sum)).T))\n",
    "\n",
    "        # compute testing errors\n",
    "        test_pos_error = pred_summary_function(test_y, fit_pos.predict(test_pos_edges_sum.reshape(-1, 1)))\n",
    "        test_neg_error = pred_summary_function(test_y, fit_neg.predict(test_neg_edges_sum.reshape(-1, 1)))\n",
    "        test_posneg_error = pred_summary_function(test_y, fit_pos_neg.predict(np.stack((test_pos_edges_sum, test_neg_edges_sum)).T))\n",
    "\n",
    "        fold_accs_train.append((train_pos_error, train_neg_error, train_posneg_error))\n",
    "        fold_accs_test.append((test_pos_error, test_neg_error, test_posneg_error))\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'CV Fold: {fold_i+1:<10} Train Pos-Edges Model Accuracy: {round(train_pos_error, 3):<10} Train Neg-Edges Model Accuracy: {round(train_neg_error, 3):<10} Train Pos/Neg-Edges Model Accuracy: {round(train_posneg_error, 3):<10}')\n",
    "            print(f'CV Fold: {fold_i+1:<10} Test  Pos-Edges Model Accuracy: {round(test_pos_error, 3):<10} Test  Neg-Edges Model Accuracy: {round(test_neg_error, 3):<10} Test  Pos/Neg-Edges Model Accuracy: {round(test_posneg_error, 3):<10}')\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(13, 7))\n",
    "    plt.plot(range(1, len(fold_accs_train)+1), [x[0] for x in fold_accs_train], linestyle='--', marker='o', color='C0', label='Train Pos-Edges Model')\n",
    "    plt.plot(range(1, len(fold_accs_train)+1), [x[1] for x in fold_accs_train], linestyle='--', marker='o', color='C1', label='Train Neg-Edges Model')\n",
    "    plt.plot(range(1, len(fold_accs_train)+1), [x[2] for x in fold_accs_train], linestyle='--', marker='o', color='C2', label='Train Pos/Neg-Edges Model')\n",
    "    \n",
    "    plt.plot(range(1, len(fold_accs_test)+1), [x[0] for x in fold_accs_test], linestyle='-', marker='o', color='C0', label='Test  Pos-Edges Model')\n",
    "    plt.plot(range(1, len(fold_accs_test)+1), [x[1] for x in fold_accs_test], linestyle='-', marker='o', color='C1', label='Test  Neg-Edges Model')\n",
    "    plt.plot(range(1, len(fold_accs_test)+1), [x[2] for x in fold_accs_test], linestyle='-', marker='o', color='C2', label='Test  Pos/Neg-Edges Model')\n",
    "    \n",
    "    plt.title(pred_summary_function.__name__, fontsize=20)\n",
    "    plt.xticks(range(1, len(fold_accs_test)+1))\n",
    "    plt.xlabel('CV Fold')\n",
    "    plt.legend(fontsize=20)\n",
    "    plt.show()\n",
    "    \n",
    "    return fold_accs_train, fold_accs_test"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T21:52:57.609794Z",
     "start_time": "2020-02-27T21:52:57.544269Z"
    },
    "code_folding": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fold_accs_train, fold_accs_test = cpm(all_subs_T1_flat, ages, p_threshold=0.01, n_cv_splits=5, pred_summary_function=metrics.mean_absolute_error, verbose=True)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T18:52:37.940960Z",
     "start_time": "2020-02-28T18:49:39.781990Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lasso (Linear Regression + L1 Regularization)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# LassoCV uses coordinate descent to select hyperparameter alpha \n",
    "alpha_grid = np.array([10**a for a in np.arange(-3, 3, 0.25)])\n",
    "lassoCV_model = linear_model.LassoCV(cv=5, n_alphas=len(alpha_grid), alphas=alpha_grid, fit_intercept=True, normalize=False, random_state=42, verbose=True, n_jobs=5).fit(train_T1, train_phen)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T18:57:14.623765Z",
     "start_time": "2020-02-28T18:52:40.754297Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(lassoCV_model.alphas_, lassoCV_model.mse_path_, ':')\n",
    "plt.plot(lassoCV_model.alphas_, lassoCV_model.mse_path_.mean(axis=-1), color='k', marker='o', label='Mean MSE Across Folds', linewidth=2)\n",
    "plt.axvline(x=100, linestyle='--', c='r')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:23:47.189824Z",
     "start_time": "2020-02-28T20:23:46.167663Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# based on cv results above, set alpha=100\n",
    "lasso_model = linear_model.Lasso(alpha=lassoCV_model.alpha_, fit_intercept=True, normalize=False).fit(train_T1, train_phen)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T19:03:05.522140Z",
     "start_time": "2020-02-28T19:03:00.140879Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_preds_lasso_model = lasso_model.predict(train_T1)\n",
    "test_preds_lasso_model = lasso_model.predict(test_T1)\n",
    "\n",
    "train_mae = metrics.mean_absolute_error(train_phen, train_preds_lasso_model)\n",
    "test_mae = metrics.mean_absolute_error(test_phen, test_preds_lasso_model)\n",
    "\n",
    "print(f'Train MAE: {train_mae:.3f}')\n",
    "print(f'Test MAE: {test_mae:.3f}')"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T19:26:16.703261Z",
     "start_time": "2020-02-28T19:26:16.247169Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ridge (Linear Regression + L2 Regularization)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# RidgeCV uses generalized cross validation to select hyperparameter alpha \n",
    "with warnings.catch_warnings():\n",
    "    # ignore matrix decomposition errors\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    ridgeCV_model = linear_model.RidgeCV(alphas=(0.1, 1.0, 10.0), fit_intercept=True, normalize=False, cv=5).fit(train_T1, train_phen)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T19:24:20.576475Z",
     "start_time": "2020-02-28T19:23:29.159220Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ridge_alpha = ridgeCV_model.alpha_\n",
    "print(f'CV Selected Alpha = {ridge_alpha:.3f}')"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:04:28.613450Z",
     "start_time": "2020-02-28T20:04:28.599216Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ridge_model = linear_model.Ridge(alpha=ridge_alpha, fit_intercept=True, normalize=False).fit(train_T1, train_phen)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:04:35.576044Z",
     "start_time": "2020-02-28T20:04:29.018975Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_preds_ridge_model = ridge_model.predict(train_T1)\n",
    "test_preds_ridge_model = ridge_model.predict(test_T1)\n",
    "\n",
    "train_mae = metrics.mean_absolute_error(train_phen, train_preds_ridge_model)\n",
    "test_mae = metrics.mean_absolute_error(test_phen, test_preds_ridge_model)\n",
    "\n",
    "print(f'Train MAE: {train_mae:.3f}')\n",
    "print(f'Test MAE: {test_mae:.3f}')"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:04:36.305177Z",
     "start_time": "2020-02-28T20:04:35.589009Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Elastic Net (Linear Regression + L1/L2 Regularization)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# RidgeCV uses generalized cross validation to select hyperparameter alpha \n",
    "elasticnetCV_model = linear_model.ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1], cv=5, n_alphas=len(alpha_grid), alphas=alpha_grid, random_state=42, verbose=True, n_jobs=5).fit(train_T1, train_phen)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:59:20.607271Z",
     "start_time": "2020-02-28T20:25:43.581804Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(f'CV selected alpha {elasticnetCV_model.alpha_:.3f}')\n",
    "print(f'Elastic net L1 ratio {elasticnetCV_model.l1_ratio_:.3f}')"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:59:20.620899Z",
     "start_time": "2020-02-28T20:59:20.612966Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(elasticnetCV_model.alphas_, elasticnetCV_model.mse_path_[1, :, :], ':')\n",
    "plt.plot(elasticnetCV_model.alphas_, elasticnetCV_model.mse_path_[1, :, :].mean(axis=-1), color='k', marker='o', label='Mean MSE Across Folds', linewidth=2)\n",
    "plt.axvline(x=200, linestyle='--', c='r')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T21:02:22.582024Z",
     "start_time": "2020-02-28T21:02:22.050171Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "elasticnet_model = linear_model.ElasticNet(alpha=elasticnetCV_model.alpha_, l1_ratio=elasticnetCV_model.l1_ratio_, fit_intercept=True, normalize=False, random_state=42).fit(train_T1, train_phen)\n",
    "\n",
    "train_preds_en_model = elasticnet_model.predict(train_T1)\n",
    "test_preds_en_model = elasticnet_model.predict(test_T1)\n",
    "\n",
    "train_mae = metrics.mean_absolute_error(train_phen, train_preds_en_model)\n",
    "test_mae = metrics.mean_absolute_error(test_phen, test_preds_en_model)\n",
    "\n",
    "print(f'Train MAE: {train_mae:.3f}')\n",
    "print(f'Test MAE: {test_mae:.3f}')"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T21:02:35.399337Z",
     "start_time": "2020-02-28T21:02:30.121679Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **3-D CNN**"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "ZPGbaF4WFxf0"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_subs_T1.shape"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Y8r5IPOSFxf3",
    "outputId": "02465af3-8365-4ca9-83f1-db2e01e6f8e2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_subs_T1_CNN = all_subs_T1.reshape(-1,60,72,60,1)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nnMCwGm-Fxf1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_T1_CNN = all_subs_T1_CNN[train_idxs,:,:,:,:]\n",
    "test_T1_CNN = all_subs_T1_CNN[test_idxs,:,:,:,:]"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ybZz7sKWFxf6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_T1_CNN.shape"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iQqecvStFxf9",
    "outputId": "0964e506-0889-4e92-d04b-17362bdb08bb"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_T1_CNN.shape"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7UOkb3cGFxf_",
    "outputId": "7bf2b49c-4bf3-46a8-c44d-70f46c3c0890"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "image_shape = train_T1_CNN[0].shape"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YCaeMbfsFxgC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = Sequential() # The simplest model, a linear stack of layers\n",
    "\n",
    "model.add(Conv3D(filters=64,\n",
    "                 kernel_size=(3,3,3), #determines the width, height, depth of the 3D convolution window\n",
    "                 activation='elu', #Exponential Linear Unit\n",
    "                 strides=1,\n",
    "                 padding='same',\n",
    "                 kernel_initializer='glorot_uniform', \n",
    "                 input_shape=image_shape)) # only the first layer needs to be told this info\n",
    "model.add(Conv3D(filters=64, kernel_size=(3,3,3), activation='elu', strides=(1,1,1), padding='same'))\n",
    "model.add(Conv3D(filters=64, kernel_size=(3,3,3), activation='elu', strides=(1,1,1), padding='same'))\n",
    "model.add(MaxPooling3D((2,2,2),strides=(2,2,2))) # pooling is also referred to as a downsampling layer\n",
    "model.add(BatchNormalization()) # Normalize the activations of the previous layer at each batch (aka make the mean activation close to 0 and the activation standard deviation close to 1)\n",
    "\n",
    "model.add(Conv3D(filters=32, kernel_size=(3,3,3), activation='elu', strides=(1,1,1), padding='same'))\n",
    "model.add(Conv3D(filters=32, kernel_size=(3,3,3), activation='elu', strides=(1,1,1), padding='same'))\n",
    "model.add(MaxPooling3D((2,2,2),strides=(2,2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv3D(filters=16, kernel_size=(3,3,3), activation='elu', strides=(1,1,1), padding='same'))\n",
    "model.add(Conv3D(filters=16, kernel_size=(3,3,3), activation='elu', strides=(1,1,1), padding='same'))\n",
    "model.add(MaxPooling3D((2,2,2),strides=(2,2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv3D(filters=8, kernel_size=(3,3,3), activation='elu', strides=(1,1,1), padding='same'))\n",
    "model.add(Conv3D(filters=8, kernel_size=(3,3,3), activation='elu', strides=(1,1,1), padding='same'))\n",
    "model.add(MaxPooling3D((2,2,2),strides=(2,2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(AveragePooling3D((2,2,2),strides=(2,2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu',name='features')) #convert the output of the convolutional part of the CNN into a 1D feature vector. Length of vector = n_classes\n",
    "model.add(Dense(1)) # final output is a single number (Age in this model)\n",
    "model.summary()\n",
    "\n",
    "filename=\"best_weights.h5\"\n",
    "filename2=\"weights.{epoch:02d}-{val_loss:.2f}.hdf5\""
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "id": "5rXDDliUFxgE",
    "outputId": "1f61b637-094c-464d-add1-fd3342be6ca1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "checkpoints = []\n",
    "\n",
    "if not os.path.exists('3DCNN_Results_g/'):\n",
    "    os.makedirs('3DCNN_Results_g/')\n",
    "\n",
    "checkpoints.append(ModelCheckpoint('3DCNN_Results_g/'+filename, \n",
    "                                   monitor='val_loss', \n",
    "                                   verbose=1, \n",
    "                                   save_best_only=True, \n",
    "                                   save_weights_only=True, \n",
    "                                   mode='auto', \n",
    "                                   period=1))\n",
    "\n",
    "checkpoints.append(ModelCheckpoint('3DCNN_Results_g/'+filename2, \n",
    "                                   monitor='val_loss', \n",
    "                                   verbose=1, \n",
    "                                   save_best_only=False, \n",
    "                                   save_weights_only=True, \n",
    "                                   mode='auto', \n",
    "                                   period=20))\n",
    "\n",
    "checkpoints.append(TensorBoard(log_dir='3DCNN_Results_g/TensorBoardLogs', \n",
    "                               histogram_freq=0, \n",
    "                               write_graph=True, \n",
    "                               write_images=False, \n",
    "                               embeddings_freq=0, \n",
    "                               embeddings_layer_names=['features'], \n",
    "                               embeddings_metadata='metadata.tsv'))\n",
    "#Early Stopping here is set so that if the MSE in the validation set does not improve after 10 epochs, training will stop\n",
    "checkpoints.append(EarlyStopping(monitor='val_loss', mode='auto', min_delta=0, patience=10))\n",
    "checkpoints.append(ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0))\n",
    "checkpoints.append(CSVLogger('3DCNN_Results_g/log.csv'))"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O1MWZwDjFxgG"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.compile(loss='mse', # the objective that the model will try to minimize, Mean Square Error in this model\n",
    "              optimizer='adam', \n",
    "              metrics=['mae']) # add in any other metrics you want to use to show performance of the model"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q8-MFOaOFxgJ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check available GPUs\n",
    "K.tensorflow_backend._get_available_gpus()"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "rVX5w2zHFxgL",
    "outputId": "59acdeea-27b2-41c3-8285-cfdc1e47e4c8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "NUM_EPOCHS = 500 # defines for how many times the training will repeat. 1 epoch is 1 forward pass and 1 backward pass over all the training examples\n",
    "BATCH_SIZE = 20 # the number of training examples in one forward/backward pass (or for 1 epoch)\n",
    "history1 = model.fit(train_T1_CNN, train_phen, \n",
    "          validation_split = 0.1, # This sets how much of the training data should be used as the validation set (test set during training) \n",
    "          epochs = NUM_EPOCHS,\n",
    "          batch_size= BATCH_SIZE,\n",
    "          callbacks = checkpoints)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "svhgREKsFxgN",
    "outputId": "14952ba2-bd7e-46e1-fe2e-6f2e067bf420"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.evaluate(x=test_T1_CNN, y=test_phen)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "V_FbHQAqFxgP",
    "outputId": "24a86979-33cd-4898-e4b1-f9d1341eaf07"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **2-D CNN**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_taskv_CNN = PNC_taskv[PNC_train_idxs,:,:,:]\n",
    "test_taskv_CNN = PNC_taskv[PNC_test_idxs,:,:,:]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "image_shape = train_taskv_CNN[0].shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = Sequential() # The simplest model, a linear stack of layers\n",
    "\n",
    "model.add(Conv2D(filters=64,\n",
    "                 kernel_size=(3,3), #determines the width, height, depth of the 3D convolution window\n",
    "                 activation='elu', #Exponential Linear Unit\n",
    "                 strides=1,\n",
    "                 padding='same',\n",
    "                 kernel_initializer='glorot_uniform', \n",
    "                 input_shape=image_shape)) # only the first layer needs to be told this info\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), activation='elu', strides=(1,1), padding='same'))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), activation='elu', strides=(1,1), padding='same'))\n",
    "model.add(MaxPooling2D((2,2),strides=(2,2))) # pooling is also referred to as a downsampling layer\n",
    "model.add(BatchNormalization()) # Normalize the activations of the previous layer at each batch (aka make the mean activation close to 0 and the activation standard deviation close to 1)\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), activation='elu', strides=(1,1), padding='same'))\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), activation='elu', strides=(1,1), padding='same'))\n",
    "model.add(MaxPooling2D((2,2),strides=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=16, kernel_size=(3,3), activation='elu', strides=(1,1), padding='same'))\n",
    "model.add(Conv2D(filters=16, kernel_size=(3,3), activation='elu', strides=(1,1), padding='same'))\n",
    "model.add(MaxPooling2D((2,2),strides=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=8, kernel_size=(3,3), activation='elu', strides=(1,1), padding='same'))\n",
    "model.add(Conv2D(filters=8, kernel_size=(3,3), activation='elu', strides=(1,1), padding='same'))\n",
    "model.add(MaxPooling2D((2,2),strides=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(AveragePooling2D((2,2),strides=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu',name='features')) #convert the output of the convolutional part of the CNN into a 1D feature vector. Length of vector = n_classes\n",
    "model.add(Dense(1)) # final output is a single number (Age in this model)\n",
    "model.summary()\n",
    "\n",
    "filename=\"best_weights.h5\"\n",
    "filename2=\"weights.{epoch:02d}-{val_loss:.2f}.hdf5\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "checkpoints = []\n",
    "\n",
    "if not os.path.exists('2DCNN_Results_age_task/'):\n",
    "    os.makedirs('2DCNN_Results_age_task/')\n",
    "\n",
    "checkpoints.append(ModelCheckpoint('2DCNN_Results_age_task/'+filename, \n",
    "                                   monitor='val_loss', \n",
    "                                   verbose=1, \n",
    "                                   save_best_only=True, \n",
    "                                   save_weights_only=True, \n",
    "                                   mode='auto', \n",
    "                                   period=1))\n",
    "\n",
    "checkpoints.append(ModelCheckpoint('2DCNN_Results_age_task/'+filename2, \n",
    "                                   monitor='val_loss', \n",
    "                                   verbose=1, \n",
    "                                   save_best_only=False, \n",
    "                                   save_weights_only=True, \n",
    "                                   mode='auto', \n",
    "                                   period=20))\n",
    "\n",
    "checkpoints.append(TensorBoard(log_dir='2DCNN_Results_age_task/TensorBoardLogs', \n",
    "                               histogram_freq=0, \n",
    "                               write_graph=True, \n",
    "                               write_images=False, \n",
    "                               embeddings_freq=0, \n",
    "                               embeddings_layer_names=['features'], \n",
    "                               embeddings_metadata='metadata.tsv'))\n",
    "#Early Stopping here is set so that if the MSE in the validation set does not improve after 10 epochs, training will stop\n",
    "checkpoints.append(EarlyStopping(monitor='val_loss', mode='auto', min_delta=0, patience=10))\n",
    "checkpoints.append(ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0))\n",
    "checkpoints.append(CSVLogger('2DCNN_Results_age_task/log.csv'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.compile(loss='mse', # the objective that the model will try to minimize, Mean Square Error in this model\n",
    "              optimizer='adam', \n",
    "              metrics=['mae']) # add in any other metrics you want to use to show performance of the model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "NUM_EPOCHS = 500 # defines for how many times the training will repeat. 1 epoch is 1 forward pass and 1 backward pass over all the training examples\n",
    "BATCH_SIZE = 20 # the number of training examples in one forward/backward pass (or for 1 epoch)\n",
    "history1 = model.fit(train_taskv_CNN, train_PNC_phen, \n",
    "          validation_split = 0.1, # This sets how much of the training data should be used as the validation set (test set during training) \n",
    "          epochs = NUM_EPOCHS,\n",
    "          batch_size= BATCH_SIZE,\n",
    "          callbacks = checkpoints)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.evaluate(x=test_taskv_CNN, y=test_PNC_phen)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}