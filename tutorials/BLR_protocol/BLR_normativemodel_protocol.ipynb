{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D68ks6mpPxQV"
   },
   "source": [
    "# [Predictive Clinical Neuroscience Toolkit](https://github.com/amarquand/PCNtoolkit) \n",
    "# The Normative Modeling Framework for Computational Psychiatry Protocol\n",
    "## Using Bayesian Linear Regression and Multi-Site Cortical Thickness Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sekFShu5PxQY",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Created by [Saige Rutherford](https://twitter.com/being_saige) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-F8j2e7ePxQY"
   },
   "source": [
    "<div>\n",
    "<img src=\"https://github.com/predictive-clinical-neuroscience/PCNtoolkit-demo/blob/main/tutorials/BLR_protocol/Figure2.png?raw=1\" width=\"1000\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prEV5dymPxQZ"
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wuzKCq4SPxQZ"
   },
   "source": [
    "## Install necessary libraries & grab data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORn68UDCPxQZ"
   },
   "source": [
    "### Step 1.\n",
    "\n",
    "Begin by cloning the GitHub repository using the following commands. This repository contains the necessary code and example data. Then install the python packages using pip and import them into the python environment (either Google Colab or using a local python installation on your computer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XY_jbksGPxQa",
    "outputId": "c21f62cc-6ff0-4120-e16e-8620fe55c067"
   },
   "outputs": [],
   "source": [
    "! git clone https://github.com/predictive-clinical-neuroscience/PCNtoolkit-demo.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XUqJ0xDTPxQb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# set this path to the git cloned PCNtoolkit-demo repository --> Uncomment whichever line you need for either running on your own computer or on Google Colab.\n",
    "#wdir = '<path-to-my>/PCNtoolkit-demo' # if running on your own computer, use this line (change the path to match where you cloned the repository)\n",
    "wdir ='/content/PCNtoolkit-demo' # if running on Google Colab, use this line\n",
    "\n",
    "os.chdir(os.path.join(wdir,'tutorials','BLR_protocol'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z05J1GqFPxQb",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wDmki54PxQc"
   },
   "source": [
    "## Prepare covariate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCEK3IokPxQc"
   },
   "source": [
    "### Step 2.\n",
    "\n",
    "The data set (downloaded in Step 1) includes a multi-site dataset from the [Human Connectome Project Young Adult study](https://www.humanconnectome.org/study/hcp-young-adult) and [IXI](https://brain-development.org/ixi-dataset/). It is also possible to use different datasets (i.e., your own data or additional public datasets) in this step. If using your own data here, it is recommended to load the example data to view the column names in order to match your data to this format. Read in the data files using pandas, then merge the covariate (age & sex) data from each site into a single data frame (named cov). The columns of this covariate data frame represent the predictor variables. Additional columns may be added here, depending on the research question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vVyr2UrgPxQd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joypy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pcntoolkit.normative import estimate, evaluate\n",
    "from pcntoolkit.util.utils import create_bspline_basis, compute_MSLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G6VQMkq-PxQd"
   },
   "outputs": [],
   "source": [
    "# if running in Google colab, remove the \"data/\" folder from the path\n",
    "hcp = pd.read_csv(os.path.join(wdir,'data','HCP1200_age_gender.csv'))\n",
    "ixi = pd.read_csv(os.path.join(wdir,'data','IXI_age_gender.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OgM-nnT6PxQd",
    "outputId": "9dd864b5-207c-4f0c-ff22-402e9db9cad1"
   },
   "outputs": [],
   "source": [
    "cov = pd.merge(hcp, ixi, on=[\"participant_id\", \"age\", \"sex\", \"site\"], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gkaf63EfPxQe"
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.5, style='darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "ao2iVSIVPxQe",
    "outputId": "04ae99fb-9cd6-4a87-d562-946290296657"
   },
   "outputs": [],
   "source": [
    "sns.displot(cov, x=\"age\", hue=\"site\", multiple=\"stack\", height=6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "shVeUZAZPxQe",
    "outputId": "6e9ba18c-1ce3-4003-c053-0ca9a6c320ad"
   },
   "outputs": [],
   "source": [
    "cov.groupby(['site']).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPUxTt-zPxQf"
   },
   "source": [
    "## Preprare brain data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPl50oOqPxQf"
   },
   "source": [
    "### Step 3.\n",
    "\n",
    "Next, format and combine the MRI data using the following commands. The example data contains cortical thickness maps estimated by running recon-all from Freesurfer (version 6.0). The dimensionality of the data was reduced by using ROIs from the Desikan-Killiany atlas. Including the Euler number as a covariate is also recommended, as this is a proxy metric for data quality. The [Euler number](https://mathworld.wolfram.com/EulerCharacteristic.html) from each subject's recon-all output folder was extracted into a text file and is merged into the cortical thickness data frame. The Euler number is site-specific, thus, to use the same exclusion threshold across sites it is important to center the site by subtracting the site median from all subjects at a site. Then take the square root and multiply by negative one and exclude any subjects with a square root above 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTaweyNcPxQf"
   },
   "source": [
    "Here is some psuedo-code (run from a terminal in the folder that has all subject's recon-all output folders) that was used to extract these ROIs:\n",
    "\n",
    "```export SUBJECTS_DIR=/path/to/study/freesurfer_data/```\n",
    "\n",
    "```aparcstats2table --subject sub-* --hemi lh --meas thickness --tablefile HCP1200_aparc_lh_thickness.txt```\n",
    "\n",
    "```aparcstats2table --subject sub-* --hemi rh --meas thickness --tablefile HCP1200_aparc_rh_thickness.txt```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IRWlg7lMPxQg"
   },
   "outputs": [],
   "source": [
    "hcpya = pd.read_csv(os.path.join(wdir,'data','HCP1200_aparc_thickness.csv'))\n",
    "ixi = pd.read_csv(os.path.join(wdir,'data','IXI_aparc_thickness.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87MOUvN3PxQg"
   },
   "outputs": [],
   "source": [
    "brain_all = pd.merge(ixi, hcpya, how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8w7w9RYWPxQg"
   },
   "source": [
    "We extracted the euler number from each subject's recon-all output folder into a text file and we now need to format and combine these into our brain dataframe. \n",
    "\n",
    "Below is psuedo code for how we extracted the euler number from the recon-all.log for each subject. Run this from the terminal in the folder where your subjects recon-all output folders are located. This assumes that all of your subject IDs start with \"sub-\" prefix.\n",
    "\n",
    "```for i in sub-*; do if [[ -e ${i}/scripts/recon-all.log ]]; then cat ${i}/scripts/recon-all.log | grep -A 1 \"Computing euler\" > temp_log; lh_en=`cat temp_log | head -2 | tail -1 | awk -F '=' '{print $2}' | awk -F ',' '{print $1}'`; rh_en=`cat temp_log | head -2 | tail -1 | awk -F '=' '{print $3}'`; echo \"${i}, ${lh_en}, ${rh_en}\" >> euler.csv; echo ${i}; fi; done```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGvdOT0PPxQg"
   },
   "outputs": [],
   "source": [
    "hcp_euler = pd.read_csv(os.path.join(wdir,'data','hcp-ya_euler.csv'))\n",
    "ixi_euler = pd.read_csv(os.path.join(wdir,'data','ixi_euler.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DYyZpHQKPxQg"
   },
   "outputs": [],
   "source": [
    "hcp_euler['site'] = 'hcp'\n",
    "ixi_euler['site'] = 'ixi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6JPbNhc0PxQh"
   },
   "outputs": [],
   "source": [
    "hcp_euler.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "ixi_euler.replace(r'^\\s*$', np.nan, regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hiQZiCgVPxQh"
   },
   "outputs": [],
   "source": [
    "hcp_euler.dropna(inplace=True)\n",
    "ixi_euler.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iBPeQrBbPxQh"
   },
   "outputs": [],
   "source": [
    "hcp_euler['rh_euler'] = hcp_euler['rh_euler'].astype(int)\n",
    "hcp_euler['lh_euler'] = hcp_euler['lh_euler'].astype(int)\n",
    "ixi_euler['rh_euler'] = ixi_euler['rh_euler'].astype(int)\n",
    "ixi_euler['lh_euler'] = ixi_euler['lh_euler'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6P9iTq0PxQh"
   },
   "outputs": [],
   "source": [
    "df_euler = pd.merge(hcp_euler, ixi_euler, on=['participant_id', 'lh_euler', 'rh_euler', 'site'], how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S23mcO6tPxQh"
   },
   "source": [
    "Finally, we need to center the euler number for each site. The euler number is very site-specific so in order to use the same exclusion threshold across sites we need to center the site by subtracting the site median from all subjects at a site. Then we will take the square root and multiply by negative one and exclude any subjects with a square root above 10. This choice of threshold is fairly random. If possible all of your data should be visually inspected to verify that the data inclusion is not too strict or too lenient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-OgUwExEPxQh"
   },
   "outputs": [],
   "source": [
    "df_euler['avg_euler'] = df_euler[['lh_euler','rh_euler']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "ptnUX_-8PxQh",
    "outputId": "0eed8c46-f5f8-4110-f06b-116aaf607e22"
   },
   "outputs": [],
   "source": [
    "df_euler.groupby(by='site').median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qeaX-9p3PxQi"
   },
   "outputs": [],
   "source": [
    "df_euler['site_median'] = df_euler['site']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u6DdZc0cPxQi"
   },
   "outputs": [],
   "source": [
    "df_euler['site_median'] = df_euler['site_median'].replace({'hcp':-43,'ixi':-56})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1rfnqsOcPxQi"
   },
   "outputs": [],
   "source": [
    "df_euler['avg_euler_centered'] = df_euler['avg_euler'] - df_euler['site_median']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dAaD2YxxPxQi"
   },
   "outputs": [],
   "source": [
    "df_euler['avg_euler_centered_neg'] = df_euler['avg_euler_centered']*-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eNp1aHaSPxQi"
   },
   "outputs": [],
   "source": [
    "df_euler['avg_euler_centered_neg_sqrt'] = np.sqrt(np.absolute(df_euler['avg_euler_centered_neg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nxu13Y1yPxQi"
   },
   "outputs": [],
   "source": [
    "brain = pd.merge(df_euler, brain_all, on=['participant_id'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DXdpbZe2PxQi"
   },
   "outputs": [],
   "source": [
    "brain_good = brain.query('avg_euler_centered_neg_sqrt < 10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0cbSkqolPxQj"
   },
   "source": [
    "**CRITICAL STEP:** If possible, data should be visually inspected to verify that the data inclusion is not too strict or too lenient. Subjects above the Euler number threshold should be manually checked to verify and justify their exclusion due to poor data quality. This is just one approach for automated QC used by the developers of the PCNtoolkit. Other approaches such as the ENIGMA QC pipeline or UK Biobank’s QC pipeline are also viable options for automated QC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w77w8FyCPxQj"
   },
   "source": [
    "## Combine covariate & cortical thickness dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAiIjVVvPxQj"
   },
   "source": [
    "### Step 4.\t\n",
    "\n",
    "The normative modeling function requires the covariate predictors and brain features to be in separate text files. However, it is important to first (inner) merge them together, using the following commands, to confirm that the same subjects are in each file and that the rows (representing subjects) align. This requires that both data frames have ‘subject_id’ as a column name. Once this is confirmed, exclude rows with NaN values and separate the brain features and covariate predictors into their own dataframes, using the commands below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epaCOjovPxQj"
   },
   "outputs": [],
   "source": [
    "# make sure to use how=\"inner\" so that we only include subjects that have data in both the covariate and the cortical thickness files \n",
    "all_data = pd.merge(brain_good, cov, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OzHN70ywPxQj"
   },
   "outputs": [],
   "source": [
    "# Create a list of all the ROIs you want to run a normative model for (add additional names to this list if you would like to include other brain regions from the Desikan-Killian atlas)\n",
    "roi_ids = ['lh_MeanThickness_thickness',\n",
    "           'rh_MeanThickness_thickness',\n",
    "           'lh_bankssts_thickness',\n",
    "           'lh_caudalanteriorcingulate_thickness',\n",
    "           'lh_superiorfrontal_thickness',\n",
    "           'rh_superiorfrontal_thickness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eEnGF6DmPxQk"
   },
   "outputs": [],
   "source": [
    "# Remove any subjects that have NaN variables in any of the columns\n",
    "all_data.dropna(subset=roi_ids, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-x6SldKHPxQk"
   },
   "outputs": [],
   "source": [
    "all_data_features = all_data[roi_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aihok464PxQk"
   },
   "outputs": [],
   "source": [
    "all_data_covariates = all_data[['age','sex','site']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VdG79uF2PxQk"
   },
   "source": [
    "**CRITICAL STEP:** `roi_ids` is a variable that represents which brain areas will be modeled and can be used to select subsets of the data frame if you do not wish to run models for the whole brain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1BXULRkPxQk"
   },
   "source": [
    "## Add variable to model site/scanner effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fq8JlwV3PxQk"
   },
   "source": [
    "### Step 5.\t\n",
    "\n",
    "Currently, the different sites are coded in a single column (named ‘site’) and are represented as a string data type. However, the PCNtoolkit requires binary variables. Use the pandas package as follows to address this, which has a built-in function, `pd.get_dummies`, that takes in the string ‘site’ column and dummy encodes the site variable so that there is now a column for each site and the columns contain binary variables (0=not in this site, 1=present in this site)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7o0VSqO-PxQk"
   },
   "outputs": [],
   "source": [
    "all_data_covariates = pd.get_dummies(all_data_covariates, columns=['site'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E2yuYv0EPxQk"
   },
   "outputs": [],
   "source": [
    "all_data['Average_Thickness'] = all_data[['lh_MeanThickness_thickness','rh_MeanThickness_thickness']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVF2d878PxQl"
   },
   "source": [
    "## Train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAs2RTAHPxQl"
   },
   "source": [
    "### Step 6.\t\n",
    "\n",
    "In this example, we use 80% of the data for training and 20% for testing. Please carefully read the experimental design section on train/test split considerations when using your own data in this step. Using a function from scikit-learn (`train_test_split`), stratify the train/test split using the site variable to make sure that the train/test sets both contain data from all sites, using the following commands. Next, confirm that your train and test arrays are the same size (rows), using the following commands. You do not need the same size columns (subjects) in the train and test arrays, but the rows represent the covariate and responses which should be the same across train and test arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FQtMYy9WPxQl"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(all_data_covariates, all_data_features, stratify=all_data['site'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kcWH0eUPxQl"
   },
   "source": [
    "Verify that your train & test arrays are the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ln7_zLFWPxQl",
    "outputId": "f454eb14-6cd7-4e5b-c5ad-f251b209b124"
   },
   "outputs": [],
   "source": [
    "tr_cov_size = X_train.shape\n",
    "tr_resp_size = y_train.shape\n",
    "te_cov_size = X_test.shape\n",
    "te_resp_size = y_test.shape\n",
    "print(\"Train covariate size is: \", tr_cov_size)\n",
    "print(\"Test covariate size is: \", te_cov_size)\n",
    "print(\"Train response size is: \", tr_resp_size)\n",
    "print(\"Test response size is: \", te_resp_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EELPVNVsPxQl"
   },
   "source": [
    "**CRITICAL STEP:** The model would not learn the site effects if all the data from one site was only in the test set. Therefore, we stratify the train/test split using the site variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7umzpdIPxQl"
   },
   "source": [
    "### Step 7.\t\n",
    "\n",
    "When the data were split into train and test sets, the row index was not reset. This means that the row index in the train and test data frames still correspond to the full data frame (before splitting the data occurred). The test set row index informs which subjects belong to which site, and this information is needed to evaluate per site performance metrics. Resetting the row index of the train/test data frames fixes this issue. Then extract the site row indices to a list (one list per site) and create a list called `site_names` that is used to decide which sites to evaluate model performance for, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fb7EC6vQPxQm"
   },
   "outputs": [],
   "source": [
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pVFGtZL6PxQm"
   },
   "outputs": [],
   "source": [
    "# Get indices of all the subejcts in each site so that we can evaluate the test set metrics per site\n",
    "hcp_idx = X_test.index[X_test['site_hcp'] == 1].to_list()\n",
    "ixi_idx = X_test.index[X_test['site_ixi'] == 1].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJApegDXPxQm"
   },
   "outputs": [],
   "source": [
    "# Save the site indices into a single list\n",
    "sites = [hcp_idx, ixi_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_pW9zY4PxQm"
   },
   "outputs": [],
   "source": [
    "# Create a list with sites names to use in evaluating per-site metrics\n",
    "site_names = ['hcp', 'ixi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfF277S4PxQm"
   },
   "source": [
    "## Setup output directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYWrxYgSPxQm"
   },
   "source": [
    "## Step 8.\t\n",
    "\n",
    "Save each brain region to its own text file (organized in separate directories) using the following commands, because for each response variable, Y (e.g., brain region) we fit a separate normative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GgGd1hryPxQm"
   },
   "outputs": [],
   "source": [
    "for c in y_train.columns:\n",
    "    y_train[c].to_csv('resp_tr_' + c + '.txt', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ejSNPBANPxQm"
   },
   "outputs": [],
   "source": [
    "X_train.to_csv('cov_tr.txt', sep = '\\t', header=False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ddJsDnrYPxQn"
   },
   "outputs": [],
   "source": [
    "y_train.to_csv('resp_tr.txt', sep = '\\t', header=False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0QsVwPoPxQn"
   },
   "outputs": [],
   "source": [
    "for c in y_test.columns:\n",
    "    y_test[c].to_csv('resp_te_' + c + '.txt', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cv43VtXWPxQn"
   },
   "outputs": [],
   "source": [
    "X_test.to_csv('cov_te.txt', sep = '\\t', header=False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_6DoEFbPxQn"
   },
   "outputs": [],
   "source": [
    "y_test.to_csv('resp_te.txt', sep = '\\t', header=False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mnu0sJrfPxQn"
   },
   "outputs": [],
   "source": [
    "! if [[ ! -e ROI_models/ ]]; then mkdir ROI_models; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pl-fYNZsPxQn"
   },
   "outputs": [],
   "source": [
    "# Note: please change the path in the following to wdir (depending on whether you are running on colab or not)\n",
    "\n",
    "! for i in `cat /content/PCNtoolkit-demo/data/roi_dir_names`; do if [[ -e resp_tr_${i}.txt ]]; then cd ROI_models; mkdir ${i}; cd ../; cp resp_tr_${i}.txt ROI_models/${i}/resp_tr.txt; cp resp_te_${i}.txt ROI_models/${i}/resp_te.txt; cp cov_tr.txt ROI_models/${i}/cov_tr.txt; cp cov_te.txt ROI_models/${i}/cov_te.txt; fi; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "csrMx397PxQn"
   },
   "outputs": [],
   "source": [
    "# clean up files\n",
    "! rm resp_*.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_yhGxsIzPxQn"
   },
   "outputs": [],
   "source": [
    "# clean up files\n",
    "! rm cov_t*.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrOvCLK5PxQn"
   },
   "source": [
    "# Algorithm & Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7v4JoP2PxQn"
   },
   "source": [
    "## Basis expansion using B-Splines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dop9WUXnPxQo"
   },
   "source": [
    "### Step 9.\t\n",
    "\n",
    "Now, set up a B-spline basis set that allows us to perform nonlinear regression using a linear model, using the following commands. This basis is deliberately chosen to not to be too flexible so that it can only model relatively slowly varying trends. To increase the flexibility of the model you can change the parameterization (e.g., by adding knot points to the B-spline basis or increasing the order of the interpolating polynomial). Note that in the neuroimaging literature, it is more common to use a polynomial basis expansion for this. Piecewise polynomials like B-splines are superior to polynomial basis expansions because they do not introduce a global curvature. For further details on the use of B-splines see [Fraza et al](https://pubmed.ncbi.nlm.nih.gov/34798518/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xZni-v2wPxQo",
    "outputId": "b2b42f8e-c713-4c67-f137-576cb0f55b4c"
   },
   "outputs": [],
   "source": [
    "# set this path to wherever your ROI_models folder is located (where you copied all of the covariate & response text files to in Step 4)\n",
    "data_dir = os.path.join(wdir,'ROI_models')\n",
    "\n",
    "# Create a cubic B-spline basis (used for regression)\n",
    "xmin = 10#16 # xmin & xmax are the boundaries for ages of participants in the dataset\n",
    "xmax = 95#90\n",
    "B = create_bspline_basis(xmin, xmax)\n",
    "# create the basis expansion for the covariates for each of the \n",
    "for roi in roi_ids: \n",
    "    print('Creating basis expansion for ROI:', roi)\n",
    "    roi_dir = os.path.join(data_dir, roi)\n",
    "    os.chdir(roi_dir)\n",
    "    # create output dir \n",
    "    os.makedirs(os.path.join(roi_dir,'blr'), exist_ok=True)\n",
    "    # load train & test covariate data matrices\n",
    "    X_tr = np.loadtxt(os.path.join(roi_dir, 'cov_tr.txt'))\n",
    "    X_te = np.loadtxt(os.path.join(roi_dir, 'cov_te.txt'))\n",
    "    # add intercept column \n",
    "    X_tr = np.concatenate((X_tr, np.ones((X_tr.shape[0],1))), axis=1)\n",
    "    X_te = np.concatenate((X_te, np.ones((X_te.shape[0],1))), axis=1)\n",
    "    np.savetxt(os.path.join(roi_dir, 'cov_int_tr.txt'), X_tr)\n",
    "    np.savetxt(os.path.join(roi_dir, 'cov_int_te.txt'), X_te)\n",
    "    \n",
    "    # create Bspline basis set \n",
    "    Phi = np.array([B(i) for i in X_tr[:,0]])\n",
    "    Phis = np.array([B(i) for i in X_te[:,0]])\n",
    "    X_tr = np.concatenate((X_tr, Phi), axis=1)\n",
    "    X_te = np.concatenate((X_te, Phis), axis=1)\n",
    "    np.savetxt(os.path.join(roi_dir, 'cov_bspline_tr.txt'), X_tr)\n",
    "    np.savetxt(os.path.join(roi_dir, 'cov_bspline_te.txt'), X_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbFaO7pWPxQo"
   },
   "source": [
    "## Estimate normative model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdZrf78vPxQo"
   },
   "source": [
    "### Step 10.\t\n",
    "\n",
    "Set up a variable (`data_dir`) that specifies the path to the ROI directories that were created in Step 7. Initiate two empty pandas data frames where the evaluation metrics are the column names, as follows; one will be used for overall test set evaluation (`blr_metrics`) and one will be used for site-specific test set evaluation (`blr_site_metrics`). After the normative model has been estimated, these data frames will be saved as individual csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EJrly1KQPxQo"
   },
   "outputs": [],
   "source": [
    "# Create pandas dataframes with header names to save out the overall and per-site model evaluation metrics\n",
    "blr_metrics = pd.DataFrame(columns = ['ROI', 'MSLL', 'EV', 'SMSE', 'RMSE', 'Rho'])\n",
    "blr_site_metrics = pd.DataFrame(columns = ['ROI', 'site', 'MSLL', 'EV', 'SMSE', 'RMSE', 'Rho'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGPLrct5PxQo"
   },
   "source": [
    "### Step 11.\t\n",
    "\n",
    "Estimate the normative models using a for loop to iterate over brain regions. An important consideration is whether to re-scale or standardize the covariates or responses. Whilst this generally only has a minor effect on the final model accuracy, it has implications for the interpretation of models and how they are configured. If the covariates and responses are both standardized (`standardize = True`), the model will return standardized coefficients. If (as in this case) the response variables are not standardized (`standardized = False`), then the scaling both covariates and responses will be reflected in the estimated coefficients. Also, under the linear modeling approach employed here, if the coefficients are unstandardized and do not have a zero mean, it is necessary to add an intercept column to the design matrix (this is done above in step 9 (B-spline)). The estimate function uses a few specific arguments that are worthy of commenting on: \n",
    "\n",
    "    - alg = 'blr': specifies we should use Bayesian Linear Regression.  \n",
    "    - optimizer = 'powell': use Powell's derivative-free optimization method (faster in this case than L-BFGS) \n",
    "    - savemodel = False: do not write out the final estimated model to disk \n",
    "    - saveoutput = False: return the outputs directly rather than writing them to disk\n",
    "    - standardize = False: Do not standardize the covariates or response variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OL-Y-1WVPxQo"
   },
   "source": [
    "**CRITICAL STEP:** This code fragment will loop through each region of interest in the `roi_ids` list (created in step 4) using Bayesian Linear Regression and evaluate the model on the independent test set. In principle, we could estimate the normative models on the whole data matrix at once (e.g., with the response variables stored in a `n_subjects` by `n_brain_measures` NumPy array or a text file instead of saved out into separate directories). However, running the models iteratively gives some extra flexibility in that it does not require that the included subjects are the same for each of the brain measures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eFipXY_MPxQp",
    "outputId": "5733c041-8df7-4703-b90f-83d1223746a8"
   },
   "outputs": [],
   "source": [
    "# Loop through ROIs\n",
    "for roi in roi_ids: \n",
    "    print('Running ROI:', roi)\n",
    "    roi_dir = os.path.join(data_dir, roi)\n",
    "    os.chdir(roi_dir)\n",
    "     \n",
    "    # configure the covariates to use. Change *_bspline_* to *_int_* to \n",
    "    cov_file_tr = os.path.join(roi_dir, 'cov_bspline_tr.txt')\n",
    "    cov_file_te = os.path.join(roi_dir, 'cov_bspline_te.txt')\n",
    "    \n",
    "    # load train & test response files\n",
    "    resp_file_tr = os.path.join(roi_dir, 'resp_tr.txt')\n",
    "    resp_file_te = os.path.join(roi_dir, 'resp_te.txt') \n",
    "    \n",
    "    # run a basic model\n",
    "    yhat_te, s2_te, nm, Z, metrics_te = estimate(cov_file_tr, \n",
    "                                                 resp_file_tr, \n",
    "                                                 testresp=resp_file_te, \n",
    "                                                 testcov=cov_file_te, \n",
    "                                                 alg = 'blr', \n",
    "                                                 optimizer = 'powell', \n",
    "                                                 savemodel = True, \n",
    "                                                 saveoutput = False,\n",
    "                                                 standardize = False)\n",
    "    # save metrics\n",
    "    blr_metrics.loc[len(blr_metrics)] = [roi, metrics_te['MSLL'][0], metrics_te['EXPV'][0], metrics_te['SMSE'][0], metrics_te['RMSE'][0], metrics_te['Rho'][0]]\n",
    "    \n",
    "    # Compute metrics per site in test set, save to pandas df\n",
    "    # load true test data\n",
    "    X_te = np.loadtxt(cov_file_te)\n",
    "    y_te = np.loadtxt(resp_file_te)\n",
    "    y_te = y_te[:, np.newaxis] # make sure it is a 2-d array\n",
    "    \n",
    "    # load training data (required to compute the MSLL)\n",
    "    y_tr = np.loadtxt(resp_file_tr)\n",
    "    y_tr = y_tr[:, np.newaxis]\n",
    "    \n",
    "    for num, site in enumerate(sites):     \n",
    "        y_mean_te_site = np.array([[np.mean(y_te[site])]])\n",
    "        y_var_te_site = np.array([[np.var(y_te[site])]])\n",
    "        yhat_mean_te_site = np.array([[np.mean(yhat_te[site])]])\n",
    "        yhat_var_te_site = np.array([[np.var(yhat_te[site])]])\n",
    "        \n",
    "        metrics_te_site = evaluate(y_te[site], yhat_te[site], s2_te[site], y_mean_te_site, y_var_te_site)\n",
    "        \n",
    "        site_name = site_names[num]\n",
    "        blr_site_metrics.loc[len(blr_site_metrics)] = [roi, site_names[num], metrics_te_site['MSLL'][0], metrics_te_site['EXPV'][0], metrics_te_site['SMSE'][0], metrics_te_site['RMSE'][0], metrics_te_site['Rho'][0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wc4NFsJBPxQp"
   },
   "source": [
    "# Evaluation & Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CAdaK2NPxQr"
   },
   "source": [
    "## Describe the normative model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8syuqDxPxQr"
   },
   "source": [
    "### Step 12.\t\n",
    "\n",
    "In step 11, when we looped over each region of interest in the `roi_ids` list (created in step 4) and evaluated the normative model on the independent test set, it also computed the evaluation metrics such as the explained variance, mean standardized log-loss and Pearson correlation between true and predicted test responses. The evaluation metrics were calculated for the full test set and calculated separately for each scanning site. The metrics were saved out to a csv file. In this step we load the evaluation metrics into a panads data frame and use the describe function to show the range, mean, and standard deviation of each of the evaluation metrics. Table 2 shows how to interpret the ranges/directions of good model fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U3ncOg8FPxQs",
    "outputId": "46ada4f1-1b74-4e9b-ad72-2cdbc097a70d"
   },
   "outputs": [],
   "source": [
    "# Overall test set evaluation metrics\n",
    "print(blr_metrics['EV'].describe())\n",
    "print(blr_metrics['MSLL'].describe())\n",
    "print(blr_metrics['SMSE'].describe())\n",
    "print(blr_metrics['Rho'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZEs7Ej4-qGi"
   },
   "source": [
    "The deviation scores are output as a text file in separate folders. We want to summarize the deviation scores across all models estimates so we can organize them into a single file, and merge the deviation scores into the original data file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvh8npp-PxQt",
    "tags": []
   },
   "source": [
    "## Visualize normative model outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXqJemw7PxQu"
   },
   "source": [
    "<div>\n",
    "<img src=\"https://github.com/predictive-clinical-neuroscience/PCNtoolkit-demo/blob/main/tutorials/BLR_protocol/Figure4.png?raw=1\" width=\"1000\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89GpR-mIPxQu"
   },
   "source": [
    "### Figure 4A viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 639
    },
    "id": "cNeOX8RJPxQu",
    "outputId": "36ef427b-335f-47ef-e7ea-908cac7695fe"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "def color_gradient(x=0.0, start=(0, 0, 0), stop=(1, 1, 1)):\n",
    "    r = np.interp(x, [0, 1], [start[0], stop[0]])\n",
    "    g = np.interp(x, [0, 1], [start[1], stop[1]])\n",
    "    b = np.interp(x, [0, 1], [start[2], stop[2]])\n",
    "    return r, g, b\n",
    "\n",
    "plt.figure(dpi=380)\n",
    "fig, axes = joypy.joyplot(blr_site_metrics, column=['EV'], overlap=2.5, by=\"site\", ylim='own', fill=True, figsize=(8,8)\n",
    "                          , legend=False, xlabels=True, ylabels=True, colormap=lambda x: color_gradient(x, start=(.08, .45, .8),stop=(.8, .34, .44))\n",
    "                          , alpha=0.6, linewidth=.5, linecolor='w', fade=True);\n",
    "plt.title('Test Set Explained Variance', fontsize=18, color='black', alpha=1)\n",
    "plt.xlabel('Explained Variance', fontsize=14, color='black', alpha=1)\n",
    "plt.ylabel('Site', fontsize=14, color='black', alpha=1)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTArq1PKPxQu"
   },
   "source": [
    "The code used to create the visualizations shown in Figure 4 panels B-F, can be found in this [notebook](https://github.com/predictive-clinical-neuroscience/PCNtoolkit-demo/blob/main/tutorials/BLR_protocol/visualizations.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTrvxDmGPxQu"
   },
   "source": [
    "## Post-Hoc analysis ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlC69kfpPxQu"
   },
   "source": [
    "The code for running SVM classification and classical case vs. control t-testing on the outputs of normative modeling can be found in this [notebook](https://github.com/predictive-clinical-neuroscience/PCNtoolkit-demo/blob/main/tutorials/BLR_protocol/post_hoc_analysis.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiATEEBdPxQv"
   },
   "source": [
    "The code for running other predictive models (regression, using the outputs of normative modeling as predictive features) can be found in this [notebook](https://github.com/predictive-clinical-neuroscience/PCNtoolkit-demo/blob/main/tutorials/BLR_protocol/other_predictive_models.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yapjr2CTPxQv"
   },
   "source": [
    "The code for transfering a pre-trained normative model to a new dataset can be found in this [notebook](https://github.com/predictive-clinical-neuroscience/PCNtoolkit-demo/blob/main/tutorials/BLR_protocol/transfer_pretrained_normative_models.ipynb)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "BLR_normativemodel_protocol.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
